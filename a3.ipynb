{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "authors: Matthew Wilson, Daniele Reda\n",
    "created: 2020/01/14\n",
    "last_updated: 2020/01/27\n",
    "-->\n",
    "\n",
    "\n",
    "## CPSC 533V: Assignment 3 - Tabular Q Learning using OpenAI Gym environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Tabular Q-Learning\n",
    "\n",
    "Tabular Q-learning is an RL algorithm for problems with discrete states and discrete actions. The algorithm is described in the class notes, which borrows the summary description from [Section 6.5](http://incompleteideas.net/book/RLbook2018.pdf#page=153) of Richard Sutton's RL book. In the tabular approach, the Q-value is represented as a lookup table. As discussed in class, Q-learning can further be extended to continuous states and discrete actions, leading to the [Atari DQN](https://arxiv.org/abs/1312.5602) / Deep Q-learning algorithm.  However, it is important and informative to first fully understand tabular Q-learning.\n",
    "\n",
    "Informally, Q-learning works as follows: The goal is to learn the optimal Q-function: \n",
    "`Q(s,a)`, which is the *value* of being at state `s` and taking action `a`.  Q tells you how well you expect to do, on average, from here on out, given that you act optimally.  Once the Q function is learned, choosing an optimal action is as simple as looping over all possible actions and choosing the one with the highest Q (optimal action $a^* = \\text{max}_a Q(s,a)$).  To learn Q, we initialize it arbitrarily and then iteratively refine it using the Bellman backup equation for Q functions, namely: \n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\text{max}_a Q(s', a) - Q(s,a)]$.\n",
    "Here, $r$ is the reward associated with with the transition from state s to s', and $\\alpha$ is a learning rate.\n",
    "\n",
    "In this assignment you will implement tabular Q-learning and apply it to CartPole -- an environment with a **continuous** state space.  To apply the tabular method, you will need to discretize the CartPole state space by dividing the state-space into bins.\n",
    "\n",
    "\n",
    "**Assignment goals:**\n",
    "- to become familiar with python/numpy, as well as using an OpenAI Gym environment\n",
    "- to understand tabular Q-learning, by implementing tabular Q-Learning for \n",
    "  a discretized version of a continuous-state environment, and experimenting with the implementation\n",
    "- (optional) to develop further intuition regarding possible variations of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Deep reinforcement learning has generated impressive results for board games ([Go][go], [Chess/Shogi][chess]), video games ([Atari][atari], , [DOTA2][dota], [StarCraft II][scii]), [and][baoding] [robotic][rubix] [control][anymal] ([of][cassie] [course][mimic] ;)).  RL is beginning to work for an increasing range of tasks and capabilities.  At the same time, there are many [gaping holes][irpan] and [difficulties][amid] in applying these methods. Understanding deep RL is important if you wish to have a good grasp of the modern landscape of control methods.\n",
    "\n",
    "These next several assignments are designed to get you started with deep reinforcement learning, to give you a more close and personal understanding of the methods, and to provide you with a good starting point from which you can branch out into topics of interest. You will implement basic versions of some of the important fundamental algorithms in this space, including Q-learning and policy gradient/search methods.\n",
    "\n",
    "We will only have time to cover a subset of methods and ideas in this space.\n",
    "If you want to dig deeper, we suggest following the links given on the course webpage.  Additionally we draw special attention to the [Sutton book](http://incompleteideas.net/book/RLbook2018.pdf) for RL fundamentals and in depth coverage, and OpenAI's [Spinning Up resources](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) for a concise intro to RL and deep RL concepts, as well as good comparisons and implementations of modern deep RL algorithms.\n",
    "\n",
    "\n",
    "[atari]: https://arxiv.org/abs/1312.5602\n",
    "[go]: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[chess]:https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go \n",
    "[dota]: https://openai.com/blog/openai-five/\n",
    "[scii]: https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
    "[baoding]: https://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/\n",
    "[rubix]: https://openai.com/blog/solving-rubiks-cube/\n",
    "[cassie]: https://www.cs.ubc.ca/~van/papers/2019-CORL-cassie/index.html\n",
    "[mimic]: https://www.cs.ubc.ca/~van/papers/2018-TOG-deepMimic/index.html\n",
    "[anymal]: https://arxiv.org/abs/1901.08652\n",
    "\n",
    "\n",
    "[irpan]: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "[amid]: http://amid.fish/reproducing-deep-rl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym  # uncomment if necesary\n",
    "#!pip install numpy\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Explore the CartPole environment [18 pts]\n",
    "\n",
    "Your first task is to familiarize yourself with the OpenAI gym interface and the [CartPole environment]( https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "by writing a simple hand-coded policy to try to solve it.  \n",
    "To begin understanding OpenAI Gym environments, [read this first](https://gym.openai.com/docs/).) \n",
    "The gym interface is very popular and you will see many algorithm implementations and \n",
    "custom environments that support it.  You may even want to use the API in your course projects, \n",
    "to define a custom environment for a task you want to solve.\n",
    "\n",
    "Below is some example code that runs a simple random policy.  You are to:\n",
    "- **run the code to see what it does**\n",
    "- **write code that chooses an action based on the observation**.  You will need to learn about the gym API and to read the CartPole documentation to figure out what the `action` and `obs` vectors mean for this environment. \n",
    "Your hand-coded policy can be arbitrary, and it should ideally do better than the random policy.  There is no single correct answer. The goal is to become familiar with `env`s.\n",
    "- **write code to print out the total reward gained by your policy in a single episode run**\n",
    "- **answer the short-response questions below** (see the TODOs for all of this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')  # you can also try LunarLander-v2, but make sure to change it back\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# To find out what the observations mean, read the CartPole documentation.\n",
    "# Uncomment the lines below, or visit the source file: \n",
    "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "#cartpole = env.unwrapped\n",
    "#cartpole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 [10pts]\n",
    "\n",
    "# runs a single episode and render it.  try running this before editing anything\n",
    "obs = env.reset()  # get first obs/state\n",
    "while True:\n",
    "    # TODO: replace this `action` with something that depends on `obs` \n",
    "    action = env.action_space.sample()  # random action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "# TODO: print out your total sum of rewards here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the questions below, look at the full [source code here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) if you haven't already.\n",
    "\n",
    "\n",
    "**1.2. [2pts] Describe the observation and action spaces of CartPole.  What do each of the values mean/do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. [2pts] What distribution is used to sample initial states? (see the `reset` function)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. [2pts] What is the termination condition, which determines if the `env` is `done`?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. [2pts] Briefly describe your policy.  What observation information does it use?  What score did you achieve (rough maximum and average)?  And how does it compare to the random policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Discretize the env [32 pts]\n",
    "\n",
    "Next, we need to discretize CartPole's continuous state space to work for tabular Q-learning.  While this is in part  a contrived usage of tabular methods, given the existence of other approaches that are designed to cope with continuous state-spaces, it is also interesting to consider whether tabular methods can be adapted more directly via discretization of the state into bins. Furthermore, tabular methods are simple, interpretabile, and can be proved to converge, and thus they still remain relevant.\n",
    "\n",
    "Your task is to discretize the state/observation space so that it is compatible with tabular Q-learning.  To do this:\n",
    "- **implement `obs_normalizer` to pass its test**\n",
    "- **implement `get_bins` to pass its test**\n",
    "- **then answer question 2.3**\n",
    "\n",
    "[map]: https://arxiv.org/abs/1504.04909\n",
    "[qd]: https://quality-diversity.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 [15pts for passing test_normed]\n",
    "def obs_normalizer(obs):\n",
    "    \"\"\"Normalize the observations between 0 and 1\n",
    "    \n",
    "    If the observation has extremely large bounds, then clip to a reasonable range before normalizing; \n",
    "    (-2,2) should work.  (It is ok if the solution is specific to CartPole)\n",
    "    \n",
    "    Args:\n",
    "        obs (np.ndarray): shape (4,) containing an observation from CartPole using the bound of the env\n",
    "    Returns:\n",
    "        normed (np.ndarray): shape (4,) where all elements are roughly uniformly mapped to the range [0, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # HINT: check out env.observation_space.high, env.observation_space.low\n",
    "    \n",
    "    # TODO: implement this function\n",
    "    raise NotImplementedError('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST 2.1\n",
    "def test_normed():\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs, _, done, _ =  env.step(env.action_space.sample())\n",
    "        normed = obs_normalizer(obs) \n",
    "        assert np.all(normed >= 0.0) and np.all(normed <= 1.0), '{} are outside of (0,1)'.format(normed)\n",
    "        if done: break\n",
    "    env.close()\n",
    "    print('Passed!')\n",
    "test_normed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 [15pts for passing test_binned]\n",
    "def get_bins(normed, num_bins):\n",
    "    \"\"\"Map normalized observations (0,1) to bin index values (0,num_bins-1)\n",
    "    \n",
    "    Args:\n",
    "        normed (np.ndarray): shape (4,) output from obs_normalizer\n",
    "        num_bins (int): how many bins to use\n",
    "    Returns:\n",
    "        binned (np.ndarray of type np.int): shape (4,) where all elements are values in range [0,num_bins-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "    raise NotImplementedError('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST 2.2\n",
    "obs = env.reset()\n",
    "env.close()\n",
    "\n",
    "def test_binned(num_bins):\n",
    "    normed = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "    binned = get_bins(normed, num_bins)\n",
    "    assert np.all(binned >= 0) and np.all(binned < num_bins), '{} supposed to be between (0, {})'.format(binned, num_bins-1)\n",
    "    assert binned.dtype == np.int, \"You should also make sure to cast your answer to int using np.int() or arr.astype(np.int)\" \n",
    "    \n",
    "test_binned(5)\n",
    "test_binned(10)\n",
    "test_binned(50)\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3. [2 pts] If your state has 4 values and each is binned into N possible bins, how many bins are needed to represent all unique possible states)?**  (yes, this is an easy question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Solve the env [30pts] \n",
    "\n",
    "Using the pseudocode below and the functions you implemented above, implement tabular Q-learning and use it to solve CartPole.\n",
    "\n",
    "We provide setup code to initialize the Q-table and give examples of interfacing with it. Write the inner and outer loops to train your algorithm.  These training loops will be similar to those deep RL approaches, so get used to writing them!\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![Sutton RL](https://i.imgur.com/mdcWVRL.png)\n",
    "\n",
    "in summary:\n",
    "- **implement Q-learning using this pseudocode and the helper code**\n",
    "- **answer the questions below**\n",
    "- **run the suggested experiments and otherwise experiment with whatever interests you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup (see last few lines for how to use the Q-table)\n",
    "\n",
    "# hyper parameters. feel free to change these as desired and experiment with different values\n",
    "num_bins = 10\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "log_n = 1000\n",
    "# epsilon greedy\n",
    "eps = 0.05  #usage: action = optimal if np.random.rand() > eps else random\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Q-table initialized to zeros.  first 4 dims are state, last dim is for action (0,1) for left,right.\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "\n",
    "# helper function to convert observation into a binned state so we can index into our Q-table\n",
    "obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "\n",
    "s = obs2bin(obs)\n",
    "\n",
    "print('Shape of Q Table: ', Q.shape) # you can imagine why tabular learning does not scale very well\n",
    "print('Original obs {} --> binned {}'.format(obs, s))\n",
    "print('Value of Q Table at that obs/state value', Q[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 [30 pts]\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments [20 pts]\n",
    "\n",
    "Given a working algorithm, you will run a few experiments.  Either make a copy of your code above to modify, or make the modifications in a way that they can be commented out or switched between (with boolean flag if statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. [10pts] $\\epsilon$-greedy.**  How sensitive are the results to the value of $\\epsilon$?   First, write down your prediction of what would happen if $\\epsilon$ is set to various values, including for example [0, 0.05, 0.25, 0.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the experiment and observe the impact on the algorithm.  Report the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. [10pts] Design your own experiment.** Design a modification that you think would either increase or reduce performance.  A simple example (which you can use) is initializing the Q-table differently, and thinking about how this might alter performance. Write down your idea, what you think might happen, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A. Extensions (optional)\n",
    "\n",
    "- plots your learning curve, using e.g., matplotlib \n",
    "- visualize the Q-table to see which values are being updated and not\n",
    "- design a better binning strategy that uses fewer bins for a better-performing policy\n",
    "- extend this approach to work on different environments (e.g., LunarLander-v2)\n",
    "- extend this approach to work on environments with continuous actions, by using a fixed set of discrete samples of the action space.  e.g., for Pendulum-v0\n",
    "- implement a simple deep learning version of this.  we will see next homework that DQN uses some tricks to make the neural network training more stable.  Experiment directly with simply replacing the Q-table with a Q-Network and train the Q-Network using gradient descent with `loss = (targets - Q(s,a))**2`, where `targets = stop_grad(R + gamma * maxa(Q(s,a))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
